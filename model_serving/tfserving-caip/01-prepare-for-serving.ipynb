{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a TensorFlow 2 model for serving with TF Serving\n",
    "\n",
    "This notebook shows how to prepare a TensorFlow 2 model for efficient serving with **TF Serving** by exposing flexible serving signatures. \n",
    "\n",
    "The inputs and outputs of the model as used during model training may not be optimal for serving. For example, in a typical training pipeline, feature engineering is performed as a separate step preceding model training and hyperparameter tuning. When serving the model, it may be more optimal to embed the feature engineering logic into the serving interface rather than require a client application to preprocess data.\n",
    "\n",
    "This notebook uses the pretrained [ResNet V2 101](https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4) image classification model from [TF Hub](https://tfhub.dev/). The demonstrated patterns and practices can be easily generalized to other types of TensorFlow 2 models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Optional, Text, Tuple\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model store paths\n",
    "\n",
    "Update the `GCS_MODEL_STORE` constant with a name of the GCS path to store the model created in the following sections of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_MODEL_STORE = 'gs://mlops-dev-workspace/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set TF Hub URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THUB_MODEL_HANDLE = 'https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4'\n",
    "IMAGENET_LABELS_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a local workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_WORKSPACE = '/tmp/workspace'\n",
    "if tf.io.gfile.exists(LOCAL_WORKSPACE):\n",
    "  print(\"Removing previous workspace artifacts...\")\n",
    "  tf.io.gfile.rmtree(LOCAL_WORKSPACE)\n",
    "\n",
    "print(\"Creating a new workspace...\")\n",
    "tf.io.gfile.makedirs(LOCAL_WORKSPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and running the ResNet v2 101 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_DOWNLOAD_PROGRESS\"] = 'True'\n",
    "\n",
    "local_savedmodel_path = hub.resolve(THUB_MODEL_HANDLE)\n",
    "\n",
    "print(local_savedmodel_path)\n",
    "!ls -la {local_savedmodel_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load(THUB_MODEL_HANDLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected input to most TF Hub TF2 image classification models, including ResNet 101, is a rank 4 tensor conforming to the following tensor specification: `tf.TensorSpec([None, height, width, 3], tf.float32)`. For the ResNet 101 model, the expected image size is `height x width = 224 x 224`. The color values for all channels are expected to be normalized to the [0, 1] range. \n",
    "\n",
    "The output of the model is a batch of logits vectors. The indices into the logits are the `num_classes = 1001` classes of the classification from the ImageNet dataset. The mapping from indices to class labels can be found in the [labels file](download.tensorflow.org/data/ImageNetLabels.txt) with class 0 for \"background\", followed by 1000 actual ImageNet classes.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "We will now test the model on a couple of JPEG images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'test_images'\n",
    "image_list = [tf.io.read_file(os.path.join(image_folder, image_path))\n",
    "         for image_path in os.listdir(image_folder)]\n",
    "\n",
    "ncolumns = len(image_list) if len(image_list) < 4 else 4\n",
    "nrows = int(len(image_list) // ncolumns)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncolumns, figsize=(10,10))\n",
    "for axis, image in zip(axes.flat[0:], image_list):\n",
    "    decoded_image = tf.image.decode_image(image)\n",
    "    axis.set_title(decoded_image.shape)\n",
    "    axis.imshow(decoded_image.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the testing images\n",
    "\n",
    "The images need to be preprocessed to conform to the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_and_scale(image, size):\n",
    "    image = tf.image.decode_image(image, expand_animations=False)\n",
    "        \n",
    "    image_height = image.shape[0]\n",
    "    image_width = image.shape[1]\n",
    "    crop_size = tf.minimum(image_height, image_width)\n",
    "    offset_height = ((image_height - crop_size) + 1) // 2\n",
    "    offset_width = ((image_width - crop_size) + 1) // 2\n",
    "        \n",
    "    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, crop_size, crop_size)\n",
    "    image = tf.cast(tf.image.resize(image, [size, size]), tf.uint8)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 224\n",
    "\n",
    "raw_images = tf.stack(image_list)\n",
    "preprocessed_images = tf.map_fn(lambda x: _decode_and_scale(x, size), raw_images, dtype=tf.uint8)\n",
    "preprocessed_images = tf.image.convert_image_dtype(preprocessed_images, tf.float32)\n",
    "print(preprocessed_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(preprocessed_images)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we invoked the model on a batch with two images, the model returned a batch of two arrays with logits. This is not a very user friendly output so we will convert it to the list of ImageNet class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download ImageNet labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = tf.keras.utils.get_file(\n",
    "    'ImageNetLabels.txt',\n",
    "    IMAGENET_LABELS_URL)\n",
    "imagenet_labels = np.array(open(labels_path).read().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the logits to class labels\n",
    "\n",
    "We will display the 5 highest ranked labels for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction in list(predictions):\n",
    "    decoded = imagenet_labels[np.argsort(prediction.numpy())[::-1][:5]]\n",
    "    print(list(decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the model for TF Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ResNet V2 101 model from TF HUB is optimized for recomposition and fine tuning. Since there are no serving signatures in the model's metadata, it cannot be served with TF Serving as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it servable, we need, at minimum, to add a serving signature describing the default inference method of the model. Since the expected inputs to our model require a relatively complex image preprocessing to be performed by the invoking client, we will also embed the preprocessing and postprocessing logic directly into the model and expose an alternative signature that accepts raw unprocessed images and returns the list of ranked class labels and associated label probabilities. \n",
    "\n",
    "This is achieved by defining a custom module class derived from the `tf.Module` base class, that encapsulates our ResNet model and extends it with a method implementing the image preprocessing and output postprocessing logic. The default method of the custom module is mapped to the default method of the base ResNet module to maintain the analogous interface. \n",
    "\n",
    "The custom module will be exported as the `SavedModel` that includes the original model, the preprocessing logic, and two serving signatures.\n",
    "\n",
    "This technique can be generalized to other scenarios where you need to extend a TensorFlow model and you have access to the serialized `SavedModel` but you don't have access to the Python code implementing the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the custom serving module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_KEY = 'labels'\n",
    "PROBABILITIES_KEY = 'probabilities'\n",
    "NUM_LABELS = 5\n",
    "\n",
    "class ServingModule(tf.Module):\n",
    "    \"\"\"\n",
    "    A custom tf.Module that adds image preprocessing and output post processing to\n",
    "    a base TF 2 image classification model from TF Hub. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model, input_size, output_labels):\n",
    "        super(ServingModule, self).__init__()\n",
    "        self._model = base_model\n",
    "        self._input_size = input_size\n",
    "        self._output_labels = tf.constant(output_labels, dtype=tf.string)\n",
    "        \n",
    "\n",
    "    def _decode_and_scale(self, raw_image):\n",
    "        \"\"\"\n",
    "        Decodes, crops, and resizes a single raw image.\n",
    "        \"\"\"\n",
    "        \n",
    "        image = tf.image.decode_image(raw_image, dtype=tf.dtypes.uint8, expand_animations=False)\n",
    "        image_shape = tf.shape(image)\n",
    "        image_height = image_shape[0]\n",
    "        image_width = image_shape[1]\n",
    "        crop_size = tf.minimum(image_height, image_width)\n",
    "        offset_height = ((image_height - crop_size) + 1) // 2\n",
    "        offset_width = ((image_width - crop_size) + 1) // 2\n",
    "        \n",
    "        image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, crop_size, crop_size)\n",
    "        image = tf.image.resize(image, [self._input_size, self._input_size])\n",
    "        image = tf.cast(image, tf.uint8)\n",
    "    \n",
    "        return image\n",
    "    \n",
    "    def _preprocess(self, raw_inputs):\n",
    "        \"\"\"\n",
    "        Preprocesses raw inputs as sent by the client.\n",
    "        \"\"\"\n",
    "        \n",
    "        # A mitigation for https://github.com/tensorflow/tensorflow/issues/28007\n",
    "        with tf.device('/cpu:0'):\n",
    "            images = tf.map_fn(self._decode_and_scale, raw_inputs, dtype=tf.uint8)\n",
    "        images = tf.image.convert_image_dtype(images, tf.float32)\n",
    "        \n",
    "        return images\n",
    "        \n",
    "    def _postprocess(self, model_outputs):\n",
    "        \"\"\"\n",
    "        Postprocesses outputs returned by the base model.\n",
    "        \"\"\"\n",
    "        \n",
    "        probabilities = tf.nn.softmax(model_outputs)\n",
    "        indices = tf.argsort(probabilities, axis=1, direction='DESCENDING')\n",
    "        \n",
    "        return {\n",
    "            LABELS_KEY: tf.gather(self._output_labels, indices, axis=-1)[:,:NUM_LABELS],\n",
    "            PROBABILITIES_KEY: tf.sort(probabilities, direction='DESCENDING')[:,:NUM_LABELS]\n",
    "        }\n",
    "        \n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None, 224, 224, 3], tf.float32)])\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        A pass-through to the base model.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._model(x)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "    def predict_labels(self, raw_images):\n",
    "        \"\"\"\n",
    "        Preprocesses inputs, calls the base model \n",
    "        and postprocesses outputs from the base model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the preprocessing handler\n",
    "        images = self._preprocess(raw_images)\n",
    "        \n",
    "        # Call the base model\n",
    "        logits = self._model(images)\n",
    "        \n",
    "        # Call the postprocessing handler\n",
    "        outputs = self._postprocess(logits)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "serving_module = ServingModule(model, 224, imagenet_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the custom serving module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = serving_module.predict_labels(raw_images)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the custom serving module as `SavedModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet_serving'\n",
    "model_version = '1'\n",
    "model_path = os.path.join(LOCAL_WORKSPACE, model_name, model_version)\n",
    "\n",
    "default_signature = serving_module.__call__.get_concrete_function()\n",
    "preprocess_signature = serving_module.predict_labels.get_concrete_function()\n",
    "signatures = {\n",
    "    'serving_default': default_signature,\n",
    "    'serving_preprocess': preprocess_signature\n",
    "}\n",
    "\n",
    "tf.saved_model.save(serving_module, model_path, signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the `SavedModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir {model_path} --tag_set serve --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the custom serving module with TF Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the exported custom serving module using the TF Serving docker image.\n",
    "\n",
    "### Start the TF Serving server:\n",
    "\n",
    "To start the TF Serving server and expose the TF Serving REST API port (8501):\n",
    "\n",
    "1. Open a JupyterLab terminal.\n",
    "2. Pull the latest TF Serving docker image.\n",
    "\n",
    "```\n",
    "docker pull tensorflow/serving\n",
    "```\n",
    "\n",
    "\n",
    "4. Set the environment variables for the model's name and the model's path. Use the value printed by the next cell for the model's path.\n",
    "\n",
    "```\n",
    "export MODEL_PATH=[YOUR_MODEL_PATH]\n",
    "export MODEL_NAME=resnet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_path[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Start the server and register your custom module for serving\n",
    "\n",
    "```\n",
    "docker run -it --rm -p 8501:8501 \\\n",
    "-v $MODEL_PATH:/models/$MODEL_NAME \\\n",
    "-e MODEL_NAME=$MODEL_NAME \\\n",
    "tensorflow/serving \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run inference by invoking the TF Serving `Predict` API.\n",
    "\n",
    "Refer to the [TF Serving REST API Reference](https://www.tensorflow.org/tfx/serving/api_rest) for more information about the API format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the model using the `serving_preprocess` signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'test_images'\n",
    "image_list = [tf.io.read_file(os.path.join(image_folder, image_path))\n",
    "         for image_path in os.listdir(image_folder)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the images using base 64 encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images_as_list = [{'b64': base64.b64encode(image.numpy()).decode('utf-8')} for image in image_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the request body and headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    'signature_name': 'serving_preprocess',\n",
    "    'instances': raw_images_as_list\n",
    "}\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the `Predict` endpoint and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = 'http://localhost:8501/v1/models/resnet:predict'\n",
    "\n",
    "response = requests.post(uri, data=json.dumps(request_body))\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the model using the `serving_default` signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the raw images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 224\n",
    "\n",
    "raw_images = tf.stack(image_list)\n",
    "preprocessed_images = tf.map_fn(lambda x: _decode_and_scale(x, size), raw_images, dtype=tf.uint8)\n",
    "preprocessed_images = tf.image.convert_image_dtype(preprocessed_images, tf.float32)\n",
    "print(preprocessed_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the request body and headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_as_list = preprocessed_images.numpy().tolist()\n",
    "\n",
    "request_body = {\n",
    "    'signature_name': 'serving_default',\n",
    "    'instances': images_as_list\n",
    "}\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the `Predict` endpoint and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = 'http://localhost:8501/v1/models/resnet:predict'\n",
    "\n",
    "response = requests.post(uri, data=json.dumps(request_body))\n",
    "\n",
    "print(str(response.json())[0:200], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the TF Serving container\n",
    "\n",
    "1. Open a new JupyterLab terminal.\n",
    "2. Get the ID of your TF Serving container\n",
    "\n",
    "```\n",
    "docker ps\n",
    "```\n",
    "3. Terminate your TF Serving container\n",
    "\n",
    "```\n",
    "docker kill [YOUR_CONTAINER_ID]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the custom serving module to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = os.path.join(LOCAL_WORKSPACE, model_name)\n",
    "\n",
    "!gsutil cp -r {local_path} {GCS_MODEL_STORE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {GCS_MODEL_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Walk through the `aipp_deploy.ipynb` notebook to learn how to deploy the custom serving module created in this notebook to **AI Platform Prediction** using TF Serving container image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
